---
title: D2L前期总结：预备知识与线性模型
date: 2025-12-10 12:00:00 +0800
categories: [AI Learning, Daily Notes]
tags: [daily-note, d2l, pytorch]
math: true
---


> **本阶段学习目标**
> 1. 掌握神经网络基本概念，理解前向 / 反向传播。
> 2. 熟悉 PyTorch 核心组件（`Tensor`、`nn.Module`、损失函数、优化器等）。
> 3. 为后续：用 MLP 完成 MNIST 手写数字分类打基础。
{: .prompt-info}
---

## 1. 预备知识：从数据到张量

### 1.1 表格数据预处理（CSV → Tensor）

用一个简单的房价示例，走了一遍典型的数据预处理流程：

> **从 CSV 到张量的最小例子**
> ```python
> import os
> import pandas as pd
> import torch
> 
> # 1）构造并读取 CSV
> data_file = os.path.join('..', 'data', 'house_tiny.csv')
> data = pd.read_csv(data_file)
> 
> # 2）缺失值填充（只对数值列求均值）
> inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
> inputs = inputs.fillna(inputs.mean(numeric_only=True))
> 
> # 3）类别特征 one-hot 编码
> inputs = pd.get_dummies(inputs, dummy_na=True)
> 
> # 4）转换为 Tensor
> X = torch.tensor(inputs.to_numpy(dtype=float))
> y = torch.tensor(outputs.to_numpy(dtype=float))
> ```
{: .prompt-tip}

记忆点：**缺失值 → 编码 → `Tensor`**，以后看到表格数据基本都可以照这个套路来。

### 1.2 线性代数与范数

用 PyTorch 熟悉了标量、向量、矩阵及其运算：

- **点积**：$\mathbf{x} \cdot \mathbf{y} = \sum_i x_i y_i$
- **矩阵–向量乘**：`torch.mv(A, x)`
- **矩阵乘法**：`torch.mm(A, B)`
- **L2 范数**：$\| \mathbf{x} \|_2 = \sqrt{\sum_i x_i^2}$

```python
import torch

x = torch.arange(4, dtype=torch.float32)    # 向量 [0,1,2,3]
y = torch.ones(4, dtype=torch.float32)      # 向量 [1,1,1,1]

dot = torch.dot(x, y)       # 点积
norm = torch.norm(x)        # L2 范数
```

这些运算后面都会出现在**前向传播**和**梯度计算**里。

### 1.3 微积分与自动微分（AutoGrad）

先用数值方法体会导数的含义，再用 PyTorch 的 `autograd` 自动求梯度：


> **典型的自动微分代码模式**
> ```python
> import torch
> 
> x = torch.arange(4.0, requires_grad=True)  # 开启梯度
> y = 2 * torch.dot(x, x)                    # y = 2 * x^T x
> y.backward()                               # 反向传播
> print(x.grad)                              # dy/dx = 4x
> 
> # 下一次反向传播前记得清零
> x.grad.zero_()
> ```
{: .prompt-info}

记住两点：
- 非标量输出需要先 `.sum()` 再 `backward()`；
- PyTorch 默认**累积梯度**，训练循环内要 `optimizer.zero_grad()` 或 `param.grad.zero_()`。

### 1.4 概率与采样

用 `torch.distributions.multinomial` 模拟掷骰子，直观理解**频率收敛到概率**的过程，为后续交叉熵、最大似然打基础。

---

## 2. 线性模型：从零到 PyTorch 封装

### 2.1 线性回归的数学形式

线性回归是最简单的神经网络：

$$
\hat{y} = \mathbf{w}^\top \mathbf{x} + b
$$

常用的损失函数是**均方误差（MSE）**：

$$
L(\mathbf{w}, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2} (\hat{y}^{(i)} - y^{(i)})^2
$$

优化方法使用**小批量随机梯度下降（mini-batch SGD）**。

### 2.2 从零实现线性回归（全流程）


> **手写线性回归：前向 + 损失 + 反向 + 更新**
> ```python
> import torch
> import random
> 
> # 1）构造数据 y = Xw + b + 噪声
> def synthetic_data(w, b, num_examples):
>     X = torch.normal(0, 1, (num_examples, len(w)))
>     y = X @ w + b
>     y += torch.normal(0, 0.01, y.shape)
>     return X, y.reshape(-1, 1)
> 
> true_w = torch.tensor([2, -3.4])
> true_b = 4.2
> features, labels = synthetic_data(true_w, true_b, 1000)
> 
> # 2）小批量数据迭代器
> def data_iter(batch_size, features, labels):
>     num_examples = len(features)
>     indices = list(range(num_examples))
>     random.shuffle(indices)
>     for i in range(0, num_examples, batch_size):
>         batch_idx = torch.tensor(indices[i: i + batch_size])
>         yield features[batch_idx], labels[batch_idx]
> 
> # 3）模型、损失和优化（自己写）
> w = torch.zeros((2, 1), requires_grad=True)
> b = torch.zeros(1, requires_grad=True)
> 
> def linreg(X, w, b):
>     return X @ w + b
> 
> def squared_loss(y_hat, y):
>     return (y_hat - y.reshape_as(y_hat)) ** 2 / 2
> 
> def sgd(params, lr, batch_size):
>     with torch.no_grad():
>         for p in params:
>             p -= lr * p.grad / batch_size
>             p.grad.zero_()
> 
> # 4）训练循环
> lr, num_epochs, batch_size = 0.03, 3, 10
> for epoch in range(num_epochs):
>     for X, y in data_iter(batch_size, features, labels):
>         l = squared_loss(linreg(X, w, b), y).sum()
>         l.backward()
>         sgd([w, b], lr, batch_size)
> 
>     with torch.no_grad():
>         train_l = squared_loss(linreg(features, w, b), labels).mean()
>         print(f"epoch {epoch+1}, loss {train_l:.6f}")
> ```
{: .prompt-tip}

这一段基本囊括了**神经网络训练的四要素**：
1. 数据与 DataLoader
2. 模型前向计算（前向传播）
3. 损失函数
4. 梯度反向传播 + 参数更新

### 2.3 使用 nn 与 optim 简洁实现

在 PyTorch 里，线性回归可以被浓缩为一个典型的“四件套”流程：

> **最小可记忆版：用 `nn.Linear` 拟合 y = 30x**
> ```python
> import torch
> from torch import nn
> 
> # 1）准备数据
> x = torch.randn(100, 1)
> y = 30 * x
> 
> # 2）定义模型
> model = nn.Linear(1, 1)
> 
> # 3）定义损失函数和优化器
> criterion = nn.MSELoss()
> optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
> 
> # 4）训练循环
> num_epochs = 500
> for epoch in range(num_epochs):
>     y_pred = model(x)              # 前向
>     loss = criterion(y_pred, y)    # 损失
> 
>     optimizer.zero_grad()          # 梯度清零
>     loss.backward()                # 反向
>     optimizer.step()               # 更新
> 
> print(model.weight, model.bias)
> ```
{: .prompt-tip}

> 这套「**数据 → 模型 → 损失 → 优化器 → 训练循环**」的骨架，后面做 MLP / CNN / Transformer 都是同一个模式，只是模型结构变复杂而已。
{: .prompt-info}

### 2.4 代码实现对比

我们尝试了两种方式实现线性回归，深刻体会了框架封装的便利性。

| 特性 | 从零实现 (Scratch) | 简洁实现 (PyTorch API) |
| :--- | :--- | :--- |
| **模型定义** | 手写 `linreg` 函数 | `nn.Linear(2, 1)` |
| **参数初始化** | `w = torch.normal(...)` | `net[0].weight.data.normal_(...)` |
| **损失函数** | 手写 `squared_loss` | `nn.MSELoss()` |
| **优化器** | 手写 `sgd` | `torch.optim.SGD(net.parameters(), lr=0.03)` |

> **简洁实现的威力**
> 使用 `nn.Sequential` 可以快速搭建层级结构，`DataLoader` 自动处理小批量数据迭代，极大地提高了开发效率。
{: .prompt-tip}

---

## 3. 数据加载与图像分类数据集

为了之后做 MNIST / Fashion-MNIST 分类，先学会用 `torchvision` 加载图像数据：

```python
import torch
import torchvision
from torch.utils.data import DataLoader
from torchvision import transforms

transform = transforms.ToTensor()  # [0,255] → [0,1] 的 float32 Tensor

train_ds = torchvision.datasets.FashionMNIST(
    root="../data/FashionMNIST/raw",
    train=True,
    transform=transform,
    download=True,
)
test_ds = torchvision.datasets.FashionMNIST(
    root="../data/FashionMNIST/raw",
    train=False,
    transform=transform,
    download=False,
)

batch_size = 256
train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)
```

`Dataset + DataLoader` 这一套，是之后所有图像 / 文本任务的标准入口。

---

## 4. 迈向多层感知机（MLP）与 MNIST

当前进度主要覆盖了：

- 数据预处理与张量操作
- 自动微分和梯度计算
- 线性模型的数学形式与完整训练流程
- 使用 `nn` 和 `optim` 快速搭建模型
- 使用 `torchvision` 加载图像分类数据集

下一步的实践目标是：


> **目标：用 MLP 完成 MNIST 手写数字分类**
{: .prompt-danger}

可以直接在上述骨架上，把模型换成一个简单的多层感知机：

```python
import torch
from torch import nn

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Flatten(),              # 28x28 → 784
            nn.Linear(28*28, 256),
            nn.ReLU(),
            nn.Linear(256, 10),        # 10 类数字
        )

    def forward(self, x):
        return self.net(x)

model = MLP()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

for epoch in range(10):
    for X, y in train_loader:
        logits = model(X)
        loss = criterion(logits, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

后续可以在这个基础上继续完善：

- 加入准确率评估函数；
- 比较不同优化器（SGD / Adam）；
- 尝试更深的网络或加入 Dropout / BatchNorm。

这篇总结主要是帮自己快速回忆：**如何从零写出一个最小可用的训练循环，并把它迁移到更复杂的模型上**。


> **学习阶段任务**
> 1. 掌握神经网络基本原理（前向/反向传播）。
> 2. 熟悉 PyTorch 核心组件（Loss, Optimizer）。
> 3. 目标：基于 PyTorch 实现 MLP 完成 MNIST 分类。
{: .prompt-info}
