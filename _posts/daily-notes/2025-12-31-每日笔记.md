---
title: 每日小记
date: 2025-12-31 12:00:00 +0800
categories: [Blogging, Daily Notes]
tags: [daily-note]
---

# 年末小结(随便写写)

今天是 2025 年的最后一天，回顾这一年，有遗憾有欢喜，最大的事儿就是推免了，从 3 月开始准备雅思考试（历经两次退考扣了 800 块 😭）希望考出 5.5，折磨了 20 天喜提 6.5，现在回头看自己太棒了！然后 6 月开始刷代码题(碎碎念：怎么没有早点发现灵神呢 😭，自己真正 code 入门还是在 7 月初)，6-7 月开始报名夏令营，7 月下旬去中科大，八月下旬去复旦，月底回 NJU 参加考试。最后最想留的 NJU 没留下来（我恨 JYY）,反而是中科大和复旦都给了优营，人生真的好 drama😂 不过人生难免有遗憾嘛，读研能换一个城市说不定有另外的体验~一年下来，最大的感触我有能力去做好各种事情，停止内耗，及时行动才是正解 🌟 最近在准备华为的机试，希望能找到第一份实习！(1 月底再来更新情况 😊)

# 华为测试准备

- 考试时间：机试 1.21，面试 1.24(猜测)

## 机试准备(+ 面试的手撕代码)

- 考试内容：选择题(单选+多选共 150)+编程题(150 + 300)
- 斩杀线: 180/600
- Task:
  - period1:12.31 - 1.6 刷完真题，熟悉编程语言
    1. 真题(0827-1217 共 19 套) 计划每天 3 套
    2. leetcode hot 100 每天 15 道左右
  - period2:1.6 晚上安排

# 华为 AI 08-27 笔记

## 选择

### 单选

9. 若某项目有  n  位选手，每两人都对战(共  C(n,2)  场)，按胜场数排名，前  m  人晋级，第  m  名并列需加赛不能保证晋级。一个选手最少需要赢几场才能确保晋级? （ceil 意思是向上取整）
   A ceil((2n − m − 1)/2)
   B ceil((m − 1)/2)
   C ceil(((m − 1)+(n − 1)+1)/(m − n + 1))
   D ceil(((m − 1)\*n)/m)
   > 考虑 m + 1 个优胜者不可能达到的次数 w
   > 优胜圈总次数（最坏）$C_{m + 1}^2$(内部) + $(m + 1)(n - m - 1)$(对外) = $(m + 1)(n - \frac{m + 1}{2})$
   > 不可能出现 $(m + 1)w > (m + 1)(n - \frac{m + 1}{2})$，解得 $w > \frac{2n - m - 1}{2}$，即 A

![20260101005108](https://cdn.jsdelivr.net/gh/bzchen1/blog-image/img/${year}/${month}/20260101005108.png)

> 法 1:直接看线性代数中的对角占优定理
> 法 2 直接算出具体的迭代矩阵 $B_{Gs}$ 和它的谱半径。
>
> 1.  **写出迭代公式**（高斯-赛德尔的特点是：算 $y$ 的时候用**最新**算出来的 $x$）：由 $5x + y = 2 \Rightarrow x_{k+1} = \frac{1}{5}(2 - y_k) = -\frac{1}{5}y_k + 0.4$;由 $x + 3y = 4 \Rightarrow y_{k+1} = \frac{1}{3}(4 - x_{k+1})$

> 2.  **把 $x_{k+1}$ 代入 $y_{k+1}$ 的式子**（这是求迭代矩阵的关键）：$y_{k+1} = \frac{1}{3}\left(4 - \left(-\frac{1}{5}y_k + 0.4\right)\right)$ 即 $y_{k+1} = \frac{1}{3}\left(3.6 + \frac{1}{5}y_k\right) = 1.2 + \frac{1}{15}y_k$
> 3.  **写成矩阵形式**：
>     $$\begin{bmatrix} x_{k+1} \\ y_{k+1} \end{bmatrix} = \begin{bmatrix} 0 & -1/5 \\ 0 & 1/15 \end{bmatrix} \begin{bmatrix} x_k \\ y_k \end{bmatrix} + \begin{bmatrix} 0.4 \\ 1.2 \end{bmatrix}$$
>     中间那个矩阵 $\begin{bmatrix} 0 & -1/5 \\ 0 & 1/15 \end{bmatrix}$ 就是迭代矩阵 $B_{Gs}$。

> 4. **求特征值**：

    这是一个上三角矩阵，特征值就是对角线上的元素：
    $\lambda_1 = 0$, $\lambda_2 = \frac{1}{15}$。

> 5.  **求谱半径**：
>     $\rho(B_{Gs}) = \max(|0|, |\frac{1}{15}|) = \frac{1}{15} \approx 0.067$

![20260101004938](https://cdn.jsdelivr.net/gh/bzchen1/blog-image/img/${year}/${month}/20260101004938.png)

>     由于特征条件独立，无需联合概率或边缘概率(D)；故正确选项为 A 和 B。

![20260101005013](https://cdn.jsdelivr.net/gh/bzchen1/blog-image/img/${year}/${month}/20260101005013.png)

> 查看机器学习模块的相关知识点：XGBoost 与决策树 CD

![20260101005029](https://cdn.jsdelivr.net/gh/bzchen1/blog-image/img/${year}/${month}/20260101005029.png)

> 查看大模型的 RAG 部分 AC

## code

## 知识

### <span id="knowledge-dl-basics">深度学习基础组件</span>

- **激活函数**

| 名称           | 代码实现 (核心逻辑)                              | 用途                          |
| :------------- | :----------------------------------------------- | :---------------------------- |
| **Sigmoid**    | `return 1 / (1 + exp(-x))`                       | 二分类输出，将值压到 (0, 1)   |
| **ReLU**       | `return max(0, x)` <br> 或 `x if x > 0 else 0`   | 隐藏层最常用，解决梯度消失    |
| **Tanh**       | `return (exp(x) - exp(-x)) / (exp(x) + exp(-x))` | 值域 (-1, 1)，零中心化        |
| **Leaky ReLU** | `return x if x > 0 else alpha * x`               | 解决 ReLU 的 Dead Neuron 问题 |

- **池化操作**(通常用于 CNN 中，用于降维)

| 名称                              | 代码实现 (核心逻辑)                 | 特征                                 |
| :-------------------------------- | :---------------------------------- | :----------------------------------- |
| 最大池化 (Max Pooling)            | `return max(window_values)`         | 取局部**最大值**，提取最显著特征     |
| 平均池化 (Avg/Mean Pooling)       | `return sum(window_values) / count` | 取局部**平均值**，保留背景信息       |
| 全局平均池化 (Global Avg Pooling) | `return mean(entire_feature_map)`   | 对**整张图**求平均，常用于最后输出前 |

- **损失函数**

| 名称                   | 代码实现                                  | 场景                    |
| :--------------------- | :---------------------------------------- | :---------------------- |
| 均方误差 (MSE)         | `return mean((y_pred - y_true)^2)`        | **回归**问题 (预测数值) |
| 交叉熵 (Cross Entropy) | `return -sum(y_true * log(y_pred))`       | **分类**问题 (多分类)   |
| 二元交叉熵 (BCE)       | `return -(y * log(p) + (1-y) * log(1-p))` | **二分类**问题          |

### <span id="knowledge-linear-algebra">线性代数基础</span>

- 实对称矩阵：特征值实数，特征向量正交 $Q^TQ = I$
- 任何矩阵的奇异值都是非负实数
- Jacobi 迭代: $x^{(k+1)} = D^{-1}(b - (L + U)x^{(k)})$ (D 是对角矩阵)
- Gauss-Seidel 迭代: $x^{(k+1)} = (D - L)^{-1}(b - Ux^{(k)})$
- $r(N(A)) + r(A) = n$
- 初等行变换：交换、倍乘、倍加 ;
- 主元列(初等行变换后每一行第一个非 0 列)、自由列($=N(A)$)
- 零空间: $Ax = 0$ 的解空间
- 基础解系: 零空间的一组基、==表示 $Ax = 0$ 所有解的极大线性无关组==
  - 对 $A$ 进行初等行变换
  - 确定主元，自由元
  - 对自由元依次赋值为 $1$，即得到通解 $x = k_1\alpha_1 + k_2\alpha_2 + ...$
- 相似矩阵 $A \sim B$:
  - 定义: 存在可逆矩阵 $P$ 使得 $B = P^{-1}AP$
  - 性质: 相似矩阵有相同的特征多项式、特征值、特征向量空间维数
- 特征值与特征向量 $x = \lambda x$
  - 求法: 解特征多项式 $|A - \lambda I| = 0$ 得 $\lambda$，再解 $(A - \lambda I)x = 0$ 得 $x$
    - 一般多重根的特征向量个数 ≤ 重数
  - ==基础解系是 $\lambda = 0$ 的特征向量，不同特征值的特征向量线性组合不是特征向量，同一特征值的特征向量线性无关==
  - A 有特征值 $\lambda_1$,f(A) 有特征值 $f(\lambda_1)$
- 迹: $tr(A) = \sum a_{ii} = \sum \lambda_{i}$，相似矩阵迹相等
  - $|A| = \prod \lambda_{i}$
- 行列式取值
  - 所有正的对角线 - 所有反的对角线；
  - 拉普拉斯展开: $D = \sum_{j=1}^n a_{ij}A_{ij}$ （这里是行展开，当然还有列展开）
    - 代数余子式 $A_{ij}$ = $(-1)^{i+j}M_{ij}$(余子式)
- 伴随矩阵: $A^* = (A_{ij})^T$，==其中 $A_{ij}$ 是 $A$ 的余子式==
- 谱半径 $\rho$:
  - **定义**：矩阵的**谱半径**（Spectral Radius）是矩阵所有特征值（Eigenvalues）中，**绝对值最大**的那个值。
  - **符号**：通常用 $\rho(B)$ 表示。
- 各种迭代法收敛的充要条件：对于任何迭代法（雅可比或高斯-赛德尔），判断它是否收敛（即能否算出正确答案），就看迭代矩阵 $B$ 的谱半径：
  - **$\rho(B) < 1$**：**收敛**（方法有效）。$\rho$ 越小，收敛越快。
  - **$\rho(B) \ge 1$**：**发散**（算不出结果，数值会越来越大或震荡）。
- **对角占优**定理：如果矩阵 $A$ 的每一行，**对角线元素的绝对值 > 其他元素绝对值之和**，那么迭代**一定收敛**。

---

| 概念                                | 分解形式          | 核心思想                    | 主要用途               |
| :---------------------------------- | :---------------- | :-------------------------- | :--------------------- |
| **奇异值**                          | $A = U\Sigma V^T$ | 提取任意矩阵的能量/拉伸程度 | 降维、压缩、去噪       |
| **LU 分解（遇到 0 主元会失败）**    | $A = L \cdot U$   | 高斯消元法的记录            | 快速解方程组、算行列式 |
| **QR 分解（施密特正交化。“任意”）** | $A = Q \cdot R$   | 正交化过程                  | 最小二乘法、求特征值   |

#### 奇异值分解 SVD:

1.  **定义与维度**

    - 对于任意矩阵 $A_{m \times n}$，存在分解 $A = U \Sigma V^T$。
    - $U$: $m \times m$ 正交矩阵 (列向量为左奇异向量)。
    - $\Sigma$: $m \times n$ 对角矩阵 (对角线上为奇异值)。
    - $V$: $n \times n$ 正交矩阵 (列向量为右奇异向量)。

2.  **与特征值分解的关系**

    - **左**奇异向量 ($U$ 的列) $\leftrightarrow$ **$A A^T$** 的特征向量。
    - **右**奇异向量 ($V$ 的列) $\leftrightarrow$ **$A^T A$** 的特征向量。
    - 奇异值 ($\sigma$) $\leftrightarrow$ $A A^T$ (或 $A^T A$) 特征值 ($\lambda$) 的平方根，即 $\sigma_i = \sqrt{\lambda_i}$。

3.  **性质**
    - **存在性**：任意矩阵都存在 SVD。
    - **非负性**：奇异值 $\sigma_i \ge 0$。
    - **唯一性**：奇异值 $\Sigma$ 唯一；矩阵 $U$ 和 $V$ **不唯一**。

#### 极大似然估计(MLE):

- **核心思想**：
  “既然这组数据（样本）发生了，那么它发生的概率应该是最大的。”
  我们需要找到一个参数 $\theta$，使得在这组参数下，**观测到当前样本的概率（似然度）最大**。

- **解题步骤**：

  1.  **写出似然函数 $L(\theta)$**：
      - 离散型：$L(\theta) = \prod P(X_i)$ （所有样本概率连乘）
      - 连续型：$L(\theta) = \prod f(x_i)$ （所有样本概率密度连乘）
  2.  **取对数 $\ln L(\theta)$**：
      - 这一步是为了把连乘变成连加，方便求导。
      - $\ln(\prod p_i) = \sum \ln p_i$
  3.  **求导数 $\frac{d \ln L}{d\theta}$**：
      - 对未知参数 $\theta$ 求一阶导数。
  4.  **令导数为 0 解方程**：
      - 解出的 $\theta$ 就是极大似然估计值。

- **常见考点**：
  - **常数项处理**：在取对数后，像 $\ln 2, \ln \sqrt{2\pi}$ 这种不含 $\theta$ 的常数，求导时直接变成 0，计算时可以直接忽略，不要被复杂的常数吓到。
  - **单调性**：有些题目（如均匀分布 $U[0, \theta]$），似然函数是单调增函数，导数不为 0。这时候最大值通常取在边界上（例如 $\theta = \max(X_i)$）。
  - **多个参数**：如果是正态分布 $N(\mu, \sigma^2)$，需要分别对 $\mu$ 和 $\sigma^2$ 求偏导，联立方程组求解。

### <span id="knowledge-llm">大模型架构</span>

#### nlp

- Tokenizer 组件. 切分(tokenization):句子 -> 小块(token) 2. 编码/索引(index):token -> 数字 ID
  **然后交由 embedding 层输出 vector，再调用 transformer**

#### Transformer

- [知乎 Transformer 模型详解（图解最完整版）](https://zhuanlan.zhihu.com/p/338817680)
- 多头自注意力机制(Multi-Head Self-Attention): 捕捉序列中不同位置之间的依赖关系
  ==**由于 Self-Attention 并行，需要给词加上词向量**==
- 位置编码(Positional Encoding): 为了让模型捕捉序列中单词的顺序信息，加入位置编码
- 归一化层(Layer Normalization)
  - 作用：在 Transformer 的每一层（Attention 层和 Feed Forward 层）之后，都会接一个 `Add & Norm`。它的作用是把数据拉回到均值为 0、方差为 1 的分布，防止梯度消失或爆炸，让模型训练更稳定、收敛更快。

![20260102143416](https://cdn.jsdelivr.net/gh/bzchen1/blog-image/img/${year}/${month}/20260102143416.png)
![20260102143441](https://cdn.jsdelivr.net/gh/bzchen1/blog-image/img/${year}/${month}/20260102143441.png)

- 常见名词
  - 增加非线性:前馈神经网络中的 Relu 激活函数。（引入非线性）
  - 减少计算量：全连接层中的 Dropout 技术、剪枝技术。（减少冗余连接）
  - 提高泛化能力：正则化技术（L2 正则化、Dropout）。（防止过拟合）
  - 加快收敛速度：批归一化（Batch Normalization）。（稳定训练过程）

#### RAG

- **核心流程**:

  1. **Indexing (索引)**: 文档 -> 切分(Chunking) -> Embedding 模型 -> 向量数据库。
  2. **Retrieval (检索)**: 用户 Query -> Embedding 模型 -> 向量相似度匹配 -> 召回 Top-K 个**文本块**。
  3. **Generation (生成)**: 将 `System Prompt + 召回的文本块 + 用户Query` 组合，输入 LLM 生成答案。

- **关键考点**:
  - **输入给 LLM 的是什么?**: 是**原始文本(Text)**，不是向量。向量仅用于检索。
  - **Embedding 模型一致性**: 用户的 Query 和数据库里的 Doc **必须使用同一个 Embedding 模型**进行编码，否则向量空间不匹配，距离计算无意义。
  - **切分策略 (Chunking)**:
    - 太小 (e.g., 32 tokens): 语义破碎，丢失上下文，检索不准。
    - 太大: 包含过多无关噪音，且容易超出 LLM 上下文窗口。
    - 常用策略: 固定大小 (e.g., 512 tokens) + 重叠 (Overlap, e.g., 50 tokens)。
  - **Tokenizer 问题**: 通用分词器处理专业术语(医疗/法律)时，容易将专有名词切碎成通用子词(sub-word)，导致语义稀释，Embedding 质量下降。

### <span id="knowledge-ml">机器学习</span>

#### Kmeans(聚类,无监督学习)

- 目标: 将数据集划分为 K 个簇，使得簇内数据点相似度最大，簇间数据点相似度最小。
- 算法步骤:
  1. 随机选择 K 个初始质心。
  2. 分配每个数据点到最近的质心，形成 K 个簇。
  3. 更新每个簇的质心为簇内所有点的均值。
  4. 重复步骤 2 和 3，直到质心不再变化或达到最大迭代次数。

#### KNN(分类,监督学习)

- 目标: 根据训练数据中的 K 个最近邻样本的类别，预测新样本的类别。
- 算法步骤:
  1. 计算新样本与训练集中所有样本的距离（常用欧氏距离）。
  2. 选择距离最近的 K 个样本。
  3. 根据这 K 个样本的类别进行投票，选择出现频率最高的类别作为新样本的预测类别。

#### SVM

- 线性 SVM 目标函数:
  $$\min \frac{1}{2} ||w||^2 + C \sum_{i=1}^{n} \xi_i$$
  $$s.t. y_i(w \cdot x_i + b) \geq 1 - \xi_i, \xi_i \geq 0$$
  ==C 越大，正则化越弱，模型越容易过拟合==
- 核函数: 将数据映射到高维空间以实现线性可分
  - 常用核函数:
    - RBF 核(径向基函数核/高斯核): $K(x, x') = exp(-\gamma ||x - x'||^2)$
      - 应用:$f(x_{new}) = \sum \alpha_i y_i K(x_{new},x_i) + b$
      - ==$\gamma$ 越大，越陡,边界越复杂，容易过拟合==
  - 多项式核: $K(x, x') = (x \cdot x' + c)^d$

#### XGBoost 与决策树

- GBDT (梯度提升决策树) : 由多棵**决策树**组成的**加法模型**。
- **工作方式 (接力赛)**:
  - 第一棵树预测一个值。
  - 第二棵树不预测原值，而是去预测**第一棵树剩下的误差 (残差)**。
  - 第三棵树去预测**第二棵树剩下的误差**。
  - ...
  - **最终结果** = 树 1 + 树 2 + 树 3 + ...
- **为什么叫"梯度"?**: 它利用**梯度下降**的原理，每次都在减少损失函数的梯度方向上建立新树（残差 $\approx$ 负梯度）。

- **XGBoost vs GBDT (高频考点)**

| 特性         | GBDT                | XGBoost                           | 备注                                                 |
| :----------- | :------------------ | :-------------------------------- | :--------------------------------------------------- |
| **导数信息** | 一阶导数 (Gradient) | **二阶导数** (Gradient + Hessian) | XGBoost 利用二阶泰勒展开，收敛更快更准               |
| **正则化**   | 无                  | **有 (L1 & L2)**                  | XGBoost 在目标函数中加入正则项，**防过拟合**能力更强 |
| **基分类器** | 只能是 CART 回归树  | CART 或 线性分类器                | XGBoost 更加灵活                                     |
| **列抽样**   | 不支持              | **支持**                          | 类似随机森林，进一步防止过拟合                       |
| **缺失值**   | 需人工处理          | **自动学习**                      | 自动寻找缺失值的最佳分裂方向                         |

- **决策树基础**
  - **基尼不纯度 (Gini)**: 值越大，样本越**混杂** (0 表示纯度最高)。CART 树分类用。
  - **信息增益 (Information Gain)**: ID3 树用。
  - **过拟合**: 树越深，在训练集表现<!-- filepath: d:\大四\华为\bzchen1.github.io\_posts\daily-notes\2025-12-31-每日笔记.md -->

---

title: 每日小记
date: 2025-12-31 12:00:00 +0800
categories: [Blogging, Daily Notes]
tags: [daily-note]

---

# 年末小结(随便写写)

今天是 2025 年的最后一天，回顾这一年，有遗憾有欢喜，最大的事儿就是推免了，从 3 月开始准备雅思考试（历经两次退考扣了 800 块 😭）希望考出 5.5，折磨了 20 天喜提 6.5，现在回头看自己太棒了！然后 6 月开始刷代码题(碎碎念：怎么没有早点发现灵神呢 😭，自己真正 code 入门还是在 7 月初)，6-7 月开始报名夏令营，7 月下旬去中科大，八月下旬去复旦，月底回 NJU 参加考试。最后最想留的 NJU 没留下来（我恨 JYY）,反而是中科大和复旦都给了优营，人生真的好 drama😂 不过人生难免有遗憾嘛，读研能换一个城市说不定有另外的体验~一年下来，最大的感触我有能力去做好各种事情，停止内耗，及时行动才是正解 🌟 最近在准备华为的机试，希望能找到第一份实习！(1 月底再来更新情况 😊)

# 华为测试准备

- 考试时间：机试 1.21，面试 1.24(猜测)

## 机试准备(+ 面试的手撕代码)

- 考试内容：选择题(单选+多选共 150)+编程题(150 + 300)
- 斩杀线: 180/600
- Task:
  - period1:12.31 - 1.6 刷完真题，熟悉编程语言
    1. 真题(0827-1217 共 19 套) 计划每天 3 套
    2. leetcode hot 100 每天 15 道左右
  - period2:1.6 晚上安排

# 华为 AI 08-27 笔记

## 选择

### 单选

9. 若某项目有  n  位选手，每两人都对战(共  C(n,2)  场)，按胜场数排名，前  m  人晋级，第  m  名并列需加赛不能保证晋级。一个选手最少需要赢几场才能确保晋级? （ceil 意思是向上取整）
   A ceil((2n − m − 1)/2)
   B ceil((m − 1)/2)
   C ceil(((m − 1)+(n − 1)+1)/(m − n + 1))
   D ceil(((m − 1)\*n)/m)
   > 考虑 m + 1 个优胜者不可能达到的次数 w
   > 优胜圈总次数（最坏）$C_{m + 1}^2$(内部) + $(m + 1)(n - m - 1)$(对外) = $(m + 1)(n - \frac{m + 1}{2})$
   > 不可能出现 $(m + 1)w > (m + 1)(n - \frac{m + 1}{2})$，解得 $w > \frac{2n - m - 1}{2}$，即 A

![20260101005108](https://cdn.jsdelivr.net/gh/bzchen1/blog-image/img/${year}/${month}/20260101005108.png)

> 法 1:直接看线性代数中的对角占优定理
> 法 2 直接算出具体的迭代矩阵 $B_{Gs}$ 和它的谱半径。
>
> 1.  **写出迭代公式**（高斯-赛德尔的特点是：算 $y$ 的时候用**最新**算出来的 $x$）：由 $5x + y = 2 \Rightarrow x_{k+1} = \frac{1}{5}(2 - y_k) = -\frac{1}{5}y_k + 0.4$;由 $x + 3y = 4 \Rightarrow y_{k+1} = \frac{1}{3}(4 - x_{k+1})$

> 2.  **把 $x_{k+1}$ 代入 $y_{k+1}$ 的式子**（这是求迭代矩阵的关键）：$y_{k+1} = \frac{1}{3}\left(4 - \left(-\frac{1}{5}y_k + 0.4\right)\right)$ 即 $y_{k+1} = \frac{1}{3}\left(3.6 + \frac{1}{5}y_k\right) = 1.2 + \frac{1}{15}y_k$
> 3.  **写成矩阵形式**：
>     $$\begin{bmatrix} x_{k+1} \\ y_{k+1} \end{bmatrix} = \begin{bmatrix} 0 & -1/5 \\ 0 & 1/15 \end{bmatrix} \begin{bmatrix} x_k \\ y_k \end{bmatrix} + \begin{bmatrix} 0.4 \\ 1.2 \end{bmatrix}$$
>     中间那个矩阵 $\begin{bmatrix} 0 & -1/5 \\ 0 & 1/15 \end{bmatrix}$ 就是迭代矩阵 $B_{Gs}$。

> 4. **求特征值**：

    这是一个上三角矩阵，特征值就是对角线上的元素：
    $\lambda_1 = 0$, $\lambda_2 = \frac{1}{15}$。

> 5.  **求谱半径**：
>     $\rho(B_{Gs}) = \max(|0|, |\frac{1}{15}|) = \frac{1}{15} \approx 0.067$

![20260101004938](https://cdn.jsdelivr.net/gh/bzchen1/blog-image/img/${year}/${month}/20260101004938.png)

>     由于特征条件独立，无需联合概率或边缘概率(D)；故正确选项为 A 和 B。

![20260101005013](https://cdn.jsdelivr.net/gh/bzchen1/blog-image/img/${year}/${month}/20260101005013.png)

> 查看机器学习模块的相关知识点：XGBoost 与决策树 CD

![20260101005029](https://cdn.jsdelivr.net/gh/bzchen1/blog-image/img/${year}/${month}/20260101005029.png)

> 查看大模型的 RAG 部分 AC

## code

## 知识

### <span id="knowledge-dl-basics">深度学习基础组件</span>

- **激活函数**

| 名称           | 代码实现 (核心逻辑)                              | 用途                          |
| :------------- | :----------------------------------------------- | :---------------------------- |
| **Sigmoid**    | `return 1 / (1 + exp(-x))`                       | 二分类输出，将值压到 (0, 1)   |
| **ReLU**       | `return max(0, x)` <br> 或 `x if x > 0 else 0`   | 隐藏层最常用，解决梯度消失    |
| **Tanh**       | `return (exp(x) - exp(-x)) / (exp(x) + exp(-x))` | 值域 (-1, 1)，零中心化        |
| **Leaky ReLU** | `return x if x > 0 else alpha * x`               | 解决 ReLU 的 Dead Neuron 问题 |

- **池化操作**(通常用于 CNN 中，用于降维)

| 名称                              | 代码实现 (核心逻辑)                 | 特征                                 |
| :-------------------------------- | :---------------------------------- | :----------------------------------- |
| 最大池化 (Max Pooling)            | `return max(window_values)`         | 取局部**最大值**，提取最显著特征     |
| 平均池化 (Avg/Mean Pooling)       | `return sum(window_values) / count` | 取局部**平均值**，保留背景信息       |
| 全局平均池化 (Global Avg Pooling) | `return mean(entire_feature_map)`   | 对**整张图**求平均，常用于最后输出前 |

- **损失函数**

| 名称                   | 代码实现                                  | 场景                    |
| :--------------------- | :---------------------------------------- | :---------------------- |
| 均方误差 (MSE)         | `return mean((y_pred - y_true)^2)`        | **回归**问题 (预测数值) |
| 交叉熵 (Cross Entropy) | `return -sum(y_true * log(y_pred))`       | **分类**问题 (多分类)   |
| 二元交叉熵 (BCE)       | `return -(y * log(p) + (1-y) * log(1-p))` | **二分类**问题          |

### <span id="knowledge-linear-algebra">线性代数基础</span>

- 实对称矩阵：特征值实数，特征向量正交 $Q^TQ = I$
- 任何矩阵的奇异值都是非负实数
- Jacobi 迭代: $x^{(k+1)} = D^{-1}(b - (L + U)x^{(k)})$ (D 是对角矩阵)
- Gauss-Seidel 迭代: $x^{(k+1)} = (D - L)^{-1}(b - Ux^{(k)})$
- $r(N(A)) + r(A) = n$
- 初等行变换：交换、倍乘、倍加 ;
- 主元列(初等行变换后每一行第一个非 0 列)、自由列($=N(A)$)
- 零空间: $Ax = 0$ 的解空间
- 基础解系: 零空间的一组基、==表示 $Ax = 0$ 所有解的极大线性无关组==
  - 对 $A$ 进行初等行变换
  - 确定主元，自由元
  - 对自由元依次赋值为 $1$，即得到通解 $x = k_1\alpha_1 + k_2\alpha_2 + ...$
- 相似矩阵 $A \sim B$:
  - 定义: 存在可逆矩阵 $P$ 使得 $B = P^{-1}AP$
  - 性质: 相似矩阵有相同的特征多项式、特征值、特征向量空间维数
- 特征值与特征向量 $x = \lambda x$
  - 求法: 解特征多项式 $|A - \lambda I| = 0$ 得 $\lambda$，再解 $(A - \lambda I)x = 0$ 得 $x$
    - 一般多重根的特征向量个数 ≤ 重数
  - ==基础解系是 $\lambda = 0$ 的特征向量，不同特征值的特征向量线性组合不是特征向量，同一特征值的特征向量线性无关==
  - A 有特征值 $\lambda_1$,f(A) 有特征值 $f(\lambda_1)$
- 迹: $tr(A) = \sum a_{ii} = \sum \lambda_{i}$，相似矩阵迹相等
  - $|A| = \prod \lambda_{i}$
- 行列式取值
  - 所有正的对角线 - 所有反的对角线；
  - 拉普拉斯展开: $D = \sum_{j=1}^n a_{ij}A_{ij}$ （这里是行展开，当然还有列展开）
    - 代数余子式 $A_{ij}$ = $(-1)^{i+j}M_{ij}$(余子式)
- 伴随矩阵: $A^* = (A_{ij})^T$，==其中 $A_{ij}$ 是 $A$ 的余子式==
- 谱半径 $\rho$:
  - **定义**：矩阵的**谱半径**（Spectral Radius）是矩阵所有特征值（Eigenvalues）中，**绝对值最大**的那个值。
  - **符号**：通常用 $\rho(B)$ 表示。
- 各种迭代法收敛的充要条件：对于任何迭代法（雅可比或高斯-赛德尔），判断它是否收敛（即能否算出正确答案），就看迭代矩阵 $B$ 的谱半径：
  - **$\rho(B) < 1$**：**收敛**（方法有效）。$\rho$ 越小，收敛越快。
  - **$\rho(B) \ge 1$**：**发散**（算不出结果，数值会越来越大或震荡）。
- **对角占优**定理：如果矩阵 $A$ 的每一行，**对角线元素的绝对值 > 其他元素绝对值之和**，那么迭代**一定收敛**。

---

| 概念                                | 分解形式          | 核心思想                    | 主要用途               |
| :---------------------------------- | :---------------- | :-------------------------- | :--------------------- |
| **奇异值**                          | $A = U\Sigma V^T$ | 提取任意矩阵的能量/拉伸程度 | 降维、压缩、去噪       |
| **LU 分解（遇到 0 主元会失败）**    | $A = L \cdot U$   | 高斯消元法的记录            | 快速解方程组、算行列式 |
| **QR 分解（施密特正交化。“任意”）** | $A = Q \cdot R$   | 正交化过程                  | 最小二乘法、求特征值   |

#### 奇异值分解 SVD:

1.  **定义与维度**

    - 对于任意矩阵 $A_{m \times n}$，存在分解 $A = U \Sigma V^T$。
    - $U$: $m \times m$ 正交矩阵 (列向量为左奇异向量)。
    - $\Sigma$: $m \times n$ 对角矩阵 (对角线上为奇异值)。
    - $V$: $n \times n$ 正交矩阵 (列向量为右奇异向量)。

2.  **与特征值分解的关系**

    - **左**奇异向量 ($U$ 的列) $\leftrightarrow$ **$A A^T$** 的特征向量。
    - **右**奇异向量 ($V$ 的列) $\leftrightarrow$ **$A^T A$** 的特征向量。
    - 奇异值 ($\sigma$) $\leftrightarrow$ $A A^T$ (或 $A^T A$) 特征值 ($\lambda$) 的平方根，即 $\sigma_i = \sqrt{\lambda_i}$。

3.  **性质**
    - **存在性**：任意矩阵都存在 SVD。
    - **非负性**：奇异值 $\sigma_i \ge 0$。
    - **唯一性**：奇异值 $\Sigma$ 唯一；矩阵 $U$ 和 $V$ **不唯一**。

#### 极大似然估计(MLE):

- **核心思想**：
  “既然这组数据（样本）发生了，那么它发生的概率应该是最大的。”
  我们需要找到一个参数 $\theta$，使得在这组参数下，**观测到当前样本的概率（似然度）最大**。

- **解题步骤**：

  1.  **写出似然函数 $L(\theta)$**：
      - 离散型：$L(\theta) = \prod P(X_i)$ （所有样本概率连乘）
      - 连续型：$L(\theta) = \prod f(x_i)$ （所有样本概率密度连乘）
  2.  **取对数 $\ln L(\theta)$**：
      - 这一步是为了把连乘变成连加，方便求导。
      - $\ln(\prod p_i) = \sum \ln p_i$
  3.  **求导数 $\frac{d \ln L}{d\theta}$**：
      - 对未知参数 $\theta$ 求一阶导数。
  4.  **令导数为 0 解方程**：
      - 解出的 $\theta$ 就是极大似然估计值。

- **常见考点**：
  - **常数项处理**：在取对数后，像 $\ln 2, \ln \sqrt{2\pi}$ 这种不含 $\theta$ 的常数，求导时直接变成 0，计算时可以直接忽略，不要被复杂的常数吓到。
  - **单调性**：有些题目（如均匀分布 $U[0, \theta]$），似然函数是单调增函数，导数不为 0。这时候最大值通常取在边界上（例如 $\theta = \max(X_i)$）。
  - **多个参数**：如果是正态分布 $N(\mu, \sigma^2)$，需要分别对 $\mu$ 和 $\sigma^2$ 求偏导，联立方程组求解。

### <span id="knowledge-llm">大模型架构</span>

#### nlp

- Tokenizer 组件. 切分(tokenization):句子 -> 小块(token) 2. 编码/索引(index):token -> 数字 ID
  **然后交由 embedding 层输出 vector，再调用 transformer**

#### Transformer

- [知乎 Transformer 模型详解（图解最完整版）](https://zhuanlan.zhihu.com/p/338817680)
- 多头自注意力机制(Multi-Head Self-Attention): 捕捉序列中不同位置之间的依赖关系
  ==**由于 Self-Attention 并行，需要给词加上词向量**==
- 位置编码(Positional Encoding): 为了让模型捕捉序列中单词的顺序信息，加入位置编码
- 归一化层(Layer Normalization)
  - 作用：在 Transformer 的每一层（Attention 层和 Feed Forward 层）之后，都会接一个 `Add & Norm`。它的作用是把数据拉回到均值为 0、方差为 1 的分布，防止梯度消失或爆炸，让模型训练更稳定、收敛更快。

![20260102143416](https://cdn.jsdelivr.net/gh/bzchen1/blog-image/img/${year}/${month}/20260102143416.png)
![20260102143441](https://cdn.jsdelivr.net/gh/bzchen1/blog-image/img/${year}/${month}/20260102143441.png)

- 常见名词
  - 增加非线性:前馈神经网络中的 Relu 激活函数。（引入非线性）
  - 减少计算量：全连接层中的 Dropout 技术、剪枝技术。（减少冗余连接）
  - 提高泛化能力：正则化技术（L2 正则化、Dropout）。（防止过拟合）
  - 加快收敛速度：批归一化（Batch Normalization）。（稳定训练过程）

#### RAG

- **核心流程**:

  1. **Indexing (索引)**: 文档 -> 切分(Chunking) -> Embedding 模型 -> 向量数据库。
  2. **Retrieval (检索)**: 用户 Query -> Embedding 模型 -> 向量相似度匹配 -> 召回 Top-K 个**文本块**。
  3. **Generation (生成)**: 将 `System Prompt + 召回的文本块 + 用户Query` 组合，输入 LLM 生成答案。

- **关键考点**:
  - **输入给 LLM 的是什么?**: 是**原始文本(Text)**，不是向量。向量仅用于检索。
  - **Embedding 模型一致性**: 用户的 Query 和数据库里的 Doc **必须使用同一个 Embedding 模型**进行编码，否则向量空间不匹配，距离计算无意义。
  - **切分策略 (Chunking)**:
    - 太小 (e.g., 32 tokens): 语义破碎，丢失上下文，检索不准。
    - 太大: 包含过多无关噪音，且容易超出 LLM 上下文窗口。
    - 常用策略: 固定大小 (e.g., 512 tokens) + 重叠 (Overlap, e.g., 50 tokens)。
  - **Tokenizer 问题**: 通用分词器处理专业术语(医疗/法律)时，容易将专有名词切碎成通用子词(sub-word)，导致语义稀释，Embedding 质量下降。

### <span id="knowledge-ml">机器学习</span>

#### Kmeans(聚类,无监督学习)

- 目标: 将数据集划分为 K 个簇，使得簇内数据点相似度最大，簇间数据点相似度最小。
- 算法步骤:
  1. 随机选择 K 个初始质心。
  2. 分配每个数据点到最近的质心，形成 K 个簇。
  3. 更新每个簇的质心为簇内所有点的均值。
  4. 重复步骤 2 和 3，直到质心不再变化或达到最大迭代次数。

#### KNN(分类,监督学习)

- 目标: 根据训练数据中的 K 个最近邻样本的类别，预测新样本的类别。
- 算法步骤:
  1. 计算新样本与训练集中所有样本的距离（常用欧氏距离）。
  2. 选择距离最近的 K 个样本。
  3. 根据这 K 个样本的类别进行投票，选择出现频率最高的类别作为新样本的预测类别。

#### SVM

- 线性 SVM 目标函数:
  $$\min \frac{1}{2} ||w||^2 + C \sum_{i=1}^{n} \xi_i$$
  $$s.t. y_i(w \cdot x_i + b) \geq 1 - \xi_i, \xi_i \geq 0$$
  ==C 越大，正则化越弱，模型越容易过拟合==
- 核函数: 将数据映射到高维空间以实现线性可分
  - 常用核函数:
    - RBF 核(径向基函数核/高斯核): $K(x, x') = exp(-\gamma ||x - x'||^2)$
      - 应用:$f(x_{new}) = \sum \alpha_i y_i K(x_{new},x_i) + b$
      - ==$\gamma$ 越大，越陡,边界越复杂，容易过拟合==
  - 多项式核: $K(x, x') = (x \cdot x' + c)^d$

#### XGBoost 与决策树

- GBDT (梯度提升决策树) : 由多棵**决策树**组成的**加法模型**。
- **工作方式 (接力赛)**:
  - 第一棵树预测一个值。
  - 第二棵树不预测原值，而是去预测**第一棵树剩下的误差 (残差)**。
  - 第三棵树去预测**第二棵树剩下的误差**。
  - ...
  - **最终结果** = 树 1 + 树 2 + 树 3 + ...
- **为什么叫"梯度"?**: 它利用**梯度下降**的原理，每次都在减少损失函数的梯度方向上建立新树（残差 $\approx$ 负梯度）。

- **XGBoost vs GBDT (高频考点)**

| 特性         | GBDT                | XGBoost                           | 备注                                                 |
| :----------- | :------------------ | :-------------------------------- | :--------------------------------------------------- |
| **导数信息** | 一阶导数 (Gradient) | **二阶导数** (Gradient + Hessian) | XGBoost 利用二阶泰勒展开，收敛更快更准               |
| **正则化**   | 无                  | **有 (L1 & L2)**                  | XGBoost 在目标函数中加入正则项，**防过拟合**能力更强 |
| **基分类器** | 只能是 CART 回归树  | CART 或 线性分类器                | XGBoost 更加灵活                                     |
| **列抽样**   | 不支持              | **支持**                          | 类似随机森林，进一步防止过拟合                       |
| **缺失值**   | 需人工处理          | **自动学习**                      | 自动寻找缺失值的最佳分裂方向                         |

- **决策树基础**
  - **基尼不纯度 (Gini)**: 值越大，样本越**混杂** (0 表示纯度最高)。CART 树分类用。
  - **信息增益 (Information Gain)**: ID3 树用。
  - **过拟合**: 树越深，在训练集表现
