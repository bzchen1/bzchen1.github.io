---
title: 多模态大模型学习计划（Stage 1）
date: 2025-12-10 20:00:00 +0800
categories: [AI Learning, Study Plan]
tags: [roadmap, study-plan, multimodal-llm]
math: false
---

> **计划背景**
> - 路线参考 `-RoadMap/README.md`。
> - 已完成：Stage 1 · 神经网络与深度学习基础（第 1 小节）。
> - 目标：在保持身心不崩的前提下，把 Stage 1 的 2 / 3 / 4 / 5 / 6 全部跑完，并留下**可见成果**（代码、笔记、博客）。
{: .prompt-info}

---

## 0. 使用说明 & 小奖励机制

> **每天最低标准（Minimum Viable Day）**
> - 至少完成 **1 个专注学习块（Focus Block）**：45–60 分钟，手机静音、只做 RoadMap 相关的事。
> - 结束后**立刻给自己一个小奖励**，哪怕今天只有这 1 个块，也算完成任务。
{: .prompt-tip}

**建议的一个学习块结构：**
- 35–45 分钟：看资料 / 写代码 / 查 bug。
- 10–15 分钟：快速整理收获（记 3 点关键词，或在博客写 3 行“今日小结”）。
- 然后给自己一个 **固定小奖励**（二选一或自定义）：
  - 一杯奶茶 / 小零食；
  - 出去楼下走 10 分钟、刷一会儿短视频 / B 站（设定闹钟，时间到就停）。

> **重要规则**：
> - 奖励必须和“完成一个 Focus Block”强绑定，而不是随便就有。
> - 当你很不想学的时候，只要求自己：**“先撑过一个块”**，而不是“今天要啃完一章书”。
{: .prompt-warning}

---

## 1. 总体时间线（6 周版本）

按 6 周来规划 Stage 1 剩余任务（2–6 小节），每周聚焦一个主题：

| 周次 | 对应 RoadMap 小节 | 主题 | 最终可见成果 |
| --- | --- | --- | --- |
| 第 1 周 | 2. 常见深度网络 | CNN / ResNet / RNN / Transformer 基本结构 & 代码走读 | 一篇《常见网络结构速查》+ 一个简易 CNN 训练脚本 |
| 第 2 周 | 3. ViT | ViT 核心思想（patch、位置编码、多头注意力） | 复现或改写一个最小 ViT 模型 forward 代码 |
| 第 3 周 | 4. CLIP | 图文对齐、对比学习损失 | 跑通官方/第三方 CLIP 推理，在小图像集上做零样本分类 |
| 第 4 周 | 5. 多模态大模型 | LLaVA / Qwen-VL 等整体结构与使用 | 至少成功跑通 1 个多模态模型在公开数据上的 demo |
| 第 5–6 周 | 6. 微调算法 | SFT / PPO / DPO 等基础概念 | 读 1–2 篇论文 + 按教程跑通一个简单的 SFT 或 RL 微调 demo |

> 每周目标不是“完全掌握”，而是**看到一个完整闭环**：从阅读 → 理解关键公式 / 结构 → 能跑一个最小 demo → 写出一篇短总结。
{: .prompt-info}

---

## 2. 第 1 周：常见深度网络结构

**目标**：能画出/解释 CNN、ResNet、RNN、Transformer 的典型结构；能用 PyTorch 写出一个最小版 CNN 并在一个小数据集上训练。

**建议安排（可自由打乱顺序，只要 7 天内完成即可）：**

- [ ] **Day 1–2：CNN & ResNet**
  - 看《动手学深度学习》/ Happy-LLM 或相关视频中 CNN/ResNet 部分。
  - 手画一张 CNN 结构图：Conv → ReLU → Pooling → FC。
  - 打开一个 PyTorch CNN 示例（如 CIFAR-10 分类），逐层在代码里加 `print(x.shape)` 看清维度变化。
  - 在博客 `daily-notes` 里写一篇：
    - CNN 的计算步骤
    - ResNet 中 `x + F(x)` 残差连接的直觉。

- [ ] **Day 3–4：RNN / LSTM / Transformer**
  - 看 Karpathy / Happy-LLM 中 RNN / Transformer 的基础部分。
  - 至少搞清楚：
    - RNN 的隐藏状态如何在时间维度上传递；
    - Transformer 的 self-attention 输入输出维度，以及 `Q, K, V` 含义。
  - 把一个公开的 `nn.Transformer` 或简单实现的 forward 过程手动画一下（层次结构图）。

- [ ] **Day 5–6：写一篇“常见网络结构速查表”**
  - 在博客新建或追加一篇 md：每种结构用下面三行描述：
    - 适用任务
    - 关键结构（1–2 句）
    - 典型 PyTorch 写法（5–10 行代码）。

- [ ] **Day 7：复盘 & 小奖励**
  - 用 30 分钟回顾一周：
    - 哪个结构最不懂？记下疑问即可，不强行搞懂。
    - 把这一周的收获写在学习计划这篇文末（可以加一个“Week 1 总结”小节）。
  - 给自己一个比平时更大的奖励：比如一顿好吃的 / 出去放松半天。

---

## 3. 第 2 周：Vision Transformer（ViT）

**目标**：理解 ViT 的核心模块，能看懂并改写一个最小 ViT 的 forward 代码。

- [ ] 阅读 ViT 论文或教程中的**图 1 / 模型结构**，搞清楚：
  - 图像如何被切成 patch；
  - patch embedding + 位置编码；
  - Transformer encoder 堆叠；
  - `CLS` token 的作用。

- [ ] 选一个简单实现（如 `vit-pytorch` 或 D2L 对应章节）：
  - 把 forward 过程抄到自己的笔记里；
  - 在关键几行旁边写中文注释（只给自己看即可）。

- [ ] 用假数据跑通：
  - 随机生成一张 `1 x 3 x 224 x 224` 的图片输入；
  - 看每一层输出的 shape，搞清楚 patch 数量、维度变化。

- [ ] 博客输出：
  - 写一篇《ViT 最小理解笔记》：
    - 只解释 3 个问题：为什么要 patch、为什么要位置编码、为什么能不用卷积也做图像分类。

完成这一周后，可以给自己安排一次稍微正式一点的奖励，比如：
- 约朋友吃饭 / 看电影；
- 买一本你想看的纸质书或一个小玩具。

---

## 4. 第 3 周：CLIP（图文对齐）

**目标**：理解 CLIP 做了什么问题（text-image 对齐），看懂其损失函数的大致形式，并跑通一次推理 demo。

- [ ] 阅读 CLIP 论文的摘要 + 方法图：
  - 关键词：对比学习、图文编码器、共同嵌入空间。

- [ ] 代码层面：
  - 使用官方或第三方实现（如 `openai/CLIP`，或其他复现）：
    - 给几张本地图（如你手机照片）和几句文字提示；
    - 看看模型认为哪句最匹配哪张图。

- [ ] 搞清楚 InfoNCE / 对比学习损失的基本形式（只需要知道：正样本拉近，负样本推远）。

- [ ] 博客输出：
  - 一篇《CLIP 直觉理解 + Demo 记录》，贴上：
    - 几张图 + 文本提示；
    - 模型打分结果；
    - 你自己的吐槽/观察。

---

## 5. 第 4 周：多模态大模型（LLaVA / Qwen-VL 等）

**目标**：至少把一个多模态大模型在本地或云端跑起来，对着图进行提问，得到答案；并大致理解它的整体架构。

- [ ] 选一个模型：比如 Qwen-VL / LLaVA / MiniCPM-V 等任意一个你设备能跑的。
- [ ] 按官方 README 搭好环境，成功跑通：
  - 对一张图提出 3–5 个问题（视觉问答 / 推理）；
  - 截几张有趣回答的截图。
- [ ] 看一眼架构图，搞清楚：
  - 图像编码器（ViT 之类）在前面；
  - 文本大模型（LLM）在后面；
  - 中间通过什么 adapter / projector 把视觉特征接进去。
- [ ] 博客输出：
  - 一篇《我的第一个多模态大模型 Demo》：
    - 环境搭建踩坑记录（以后自己再搭会快很多）；
    - 你觉得模型聪明 / 蠢的地方各举 1–2 例。

这一周结束后，你已经能**给别人现场 demo 一个多模态模型**——这本身就是找实习时很好的谈资。

---

## 6. 第 5–6 周：大模型微调算法（SFT / RL）

**目标**：
- 懂得 SFT 是什么（监督微调）；
- 知道 RL-based 方法（PPO/DPO/GRPO）大概在优化什么；
- 至少按教程跑通一个开源 LLM/多模态模型的小规模微调 demo。

**建议拆成两部分：**

- [ ] **第 5 周：概念 & 论文粗读**
  - 读 Happy-LLM 第六章的综述部分，整理出：
    - SFT、PPO、DPO 各自优化目标的一句话解释；
    - 各自优缺点各 1–2 条。
  - 选择你 RoadMap 里提到的两篇论文之一，做“粗读”：
    - 只看摘要、引言、方法图和结论，不强行啃公式；
    - 用 10 句话写在博客里：这篇论文在解决什么问题，用了什么 idea。

- [ ] **第 6 周：动手跑一个微调 demo**
  - 选一个尽量小的开源项目（如使用 LoRA 微调一个小 LLM / 多模态模型）：
    - 数据可以非常小（几百条指令），重点是把流程走通；
    - 记录训练前后在几个样例上的输出差异。
  - 博客输出：
    - 训练脚本（关键部分）；
    - loss 曲线截图或简要描述；
    - 微调前后对比示例。

---

## 7. 给未来自己的几句话

> **1. 不要期待“我学完这一章就能拿到实习”。**
>
> 你现在做的是“铺路”工作：等你开始做真正的项目（比如一个多模态推理小系统）时，这些基础会让你“理解得更快、踩坑更少”。实习机会通常会滞后几个月才体现出来。
{: .prompt-info}

> **2. 把“是否坚持记一笔产出”当作每天的评价指标，而不是“是否完全懂了”。**
> - 产出可以是：一段代码、一张结构图、一篇 200 字的小结、一篇博客更新。
> - 只要今天有一个产出，就算赢。
{: .prompt-tip}

> **3. 如果哪天完全不想学，就只做一件最小的事：**
> - 打开之前的笔记，随便补 3 句注释；
> - 或者把今天看到的一段论文/视频，用 3 句话写进这篇计划文档最后的“今日记录”区。
> - 做完就立刻给自己奖励，然后允许自己休息。
{: .prompt-warning}

你可以根据实际进度在这篇 md 的最后，按周追加小结，比如：

```markdown
## Week 1 小结
- 完成了：CNN / ResNet 代码走读 + 一篇博客。
- 仍然困惑：Transformer 的多头注意力实现细节。
- 下周想补充：把注意力部分单独手写一遍。
```

这篇计划不需要一次性严格执行到位，更像是“对现在的自己做出的一个温柔的承诺”，后面随时可以在博客里继续调整。